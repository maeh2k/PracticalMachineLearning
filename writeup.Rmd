---
title: "Prediction Assignment Writeup"
output: html_document
---

## Data import
```{r, eval=FALSE}
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```

## Data slicing
With the provided test data containing only 20 observations, I split the provided training data into three parts: 

* training (60%), to fit the models
* cross-validation (20%), to choose among the models  
* testing (20%), to estimate the out-of-sample error

```{r, eval=FALSE}
library(caret)
partition <- createDataPartition(training$classe, p=0.6, list=FALSE)
my_training <- training[partition,]
tmp <- training[-partition,]
partition <- createDataPartition(tmp$classe, p=0.5, list=FALSE)
my_validation <- tmp[partition,]
my_testing <- tmp[-partition,]
```

## Covariates
After examining the data, I manually picked the variables I deemed necessary for the prediction. I figured roll, pitch, yaw, and the total acceleration for the four sensors would suffice. Depending on my first results I planned to reevaluate this decision later on.

```{r, eval=FALSE}
my_training_sub <- subset(my_training, select=c(classe, 
                                                roll_belt, pitch_belt, yaw_belt, total_accel_belt,
                                                roll_arm, pitch_arm, yaw_arm, total_accel_arm,
                                                roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm,
                                                roll_dumbbell, pitch_dumbbell, yaw_dumbbell, total_accel_dumbbell))
```

## Model selection
I trained models using Random Forest as well as Boosting. Depending on the results I planned to combine multiple predictors.

```{r, eval=FALSE}
mod1 <- train(classe ~ ., method="gbm", data=my_training_sub)
mod2 <- train(classe ~ ., method="rf", data=my_training_sub)
pred1 <- predict(mod1, my_validation)
pred2 <- predict(mod2, my_validation)
confusionMatrix(pred1, my_validation$classe)
confusionMatrix(pred2, my_validation$classe)
```

Using the method 'gbm' resulted in an accuracy of about 98% for the cross validation data. 'rf achieved over 98% accuracy.
I considered the accuracy for 'rf' good enough for a first submission. The 20 observations were predicted correctly.

## Out of sample error
I used my testing data (20% of the provided training data) to estimate the error of the chosen model. The accuracy was measured at 98.7% [95% CI: (0.983, 0.990)]. Consequently I expect an out of sample error rate between 1% and 2%.  
